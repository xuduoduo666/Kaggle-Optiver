{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDmPBpCLx3a4"
   },
   "source": [
    "## 1 Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuNqRdKhyBzr"
   },
   "source": [
    "## 2 Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T08:00:32.114374Z",
     "iopub.status.busy": "2023-12-20T08:00:32.114006Z",
     "iopub.status.idle": "2023-12-20T08:00:36.659661Z",
     "shell.execute_reply": "2023-12-20T08:00:36.658709Z",
     "shell.execute_reply.started": "2023-12-20T08:00:32.114343Z"
    },
    "id": "aDfQwqZCqQyW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from itertools import combinations\n",
    "from warnings import simplefilter\n",
    "\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit\n",
    "import polars as pl\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from catboost import CatBoostRegressor, EShapCalcType, EFeaturesSelectionAlgorithm\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "is_offline = False\n",
    "is_train = True\n",
    "is_infer = True\n",
    "max_lookback = np.nan\n",
    "split_day = 435 #The testing data constixtute 90% = 435 / 481"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T08:00:38.253239Z",
     "iopub.status.busy": "2023-12-20T08:00:38.252868Z",
     "iopub.status.idle": "2023-12-20T08:00:56.459707Z",
     "shell.execute_reply": "2023-12-20T08:00:56.458720Z",
     "shell.execute_reply.started": "2023-12-20T08:00:38.253212Z"
    },
    "id": "GlA4mURlqQyX",
    "outputId": "29a0fa28-c497-458d-a721-03eecae76d32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5237892, 17)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\")\n",
    "df = df.dropna(subset=[\"target\"])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T08:01:08.321146Z",
     "iopub.status.busy": "2023-12-20T08:01:08.320767Z",
     "iopub.status.idle": "2023-12-20T08:01:08.334572Z",
     "shell.execute_reply": "2023-12-20T08:01:08.333418Z",
     "shell.execute_reply.started": "2023-12-20T08:01:08.321117Z"
    },
    "id": "XSvq5VkXqQyY"
   },
   "outputs": [],
   "source": [
    "#To reduce memory usage\n",
    "def reduce_mem_usage(df, verbose=0):\n",
    "    \"\"\"\n",
    "    Iterate through all numeric columns of a dataframe and modify the data type\n",
    "    to reduce memory usage.\n",
    "    \"\"\"\n",
    "\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "\n",
    "    if verbose:\n",
    "        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
    "        decrease = 100 * (start_mem - end_mem) / start_mem\n",
    "        logger.info(f\"Decreased by {decrease:.2f}%\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVAgjxFVyqW7"
   },
   "source": [
    "### 2.1 Imbalance Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T08:01:17.661563Z",
     "iopub.status.busy": "2023-12-20T08:01:17.660644Z",
     "iopub.status.idle": "2023-12-20T08:01:18.162260Z",
     "shell.execute_reply": "2023-12-20T08:01:18.161278Z",
     "shell.execute_reply.started": "2023-12-20T08:01:17.661529Z"
    },
    "id": "jFI4WK9iqQyY"
   },
   "outputs": [],
   "source": [
    "# Import Numba for just-in-time (JIT) compilation and parallel processing\n",
    "from numba import njit, prange\n",
    "\n",
    "# Function to compute triplet imbalance in parallel using Numba\n",
    "@njit(parallel=True)\n",
    "def compute_triplet_imbalance(df_values, comb_indices):\n",
    "    num_rows = df_values.shape[0]\n",
    "    num_combinations = len(comb_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "\n",
    "    # Loop through all combinations of triplets\n",
    "    for i in prange(num_combinations):\n",
    "        a, b, c = comb_indices[i]\n",
    "\n",
    "        # Loop through rows of the DataFrame\n",
    "        for j in range(num_rows):\n",
    "            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n",
    "\n",
    "            # Prevent division by zero\n",
    "            if mid_val == min_val:\n",
    "                imbalance_features[j, i] = np.nan\n",
    "            else:\n",
    "                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n",
    "\n",
    "    return imbalance_features\n",
    "\n",
    "# Function to calculate triplet imbalance for given price data and a DataFrame\n",
    "def calculate_triplet_imbalance_numba(price, df):\n",
    "    # Convert DataFrame to numpy array for Numba compatibility\n",
    "    df_values = df[price].values\n",
    "    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n",
    "\n",
    "    # Calculate the triplet imbalance using the Numba-optimized function\n",
    "    features_array = compute_triplet_imbalance(df_values, comb_indices)\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "\n",
    "    return features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T08:01:22.918119Z",
     "iopub.status.busy": "2023-12-20T08:01:22.917425Z",
     "iopub.status.idle": "2023-12-20T08:01:22.943486Z",
     "shell.execute_reply": "2023-12-20T08:01:22.942442Z",
     "shell.execute_reply.started": "2023-12-20T08:01:22.918086Z"
    },
    "id": "W__K61JUqQyZ"
   },
   "outputs": [],
   "source": [
    "# generate imbalance features\n",
    "def imbalance_features(df):\n",
    "    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "\n",
    "    # V1\n",
    "    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n",
    "    df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n",
    "    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n",
    "    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n",
    "    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n",
    "    # Create features for pairwise price imbalances\n",
    "    for c in combinations(prices, 2):\n",
    "        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n",
    "    # Calculate triplet imbalance features using the Numba-optimized function\n",
    "    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n",
    "        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n",
    "        df[triplet_feature.columns] = triplet_feature.values\n",
    "\n",
    "\n",
    "    # V2\n",
    "    df[\"stock_weights\"] = df[\"stock_id\"].map(weights)\n",
    "    df[\"weighted_wap\"] = df[\"stock_weights\"] * df[\"wap\"]\n",
    "    df['wap_momentum'] = df.groupby('stock_id')['weighted_wap'].pct_change(periods=6)\n",
    "    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n",
    "    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n",
    "    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n",
    "    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n",
    "    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n",
    "    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n",
    "    df['spread_depth_ratio'] = (df['ask_price'] - df['bid_price']) / (df['bid_size'] + df['ask_size'])\n",
    "    df['mid_price_movement'] = df['mid_price'].diff(periods=5).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
    "    df['micro_price'] = ((df['bid_price'] * df['ask_size']) + (df['ask_price'] * df['bid_size'])) / (df['bid_size'] + df['ask_size'])\n",
    "    df['relative_spread'] = (df['ask_price'] - df['bid_price']) / df['wap']\n",
    "\n",
    "    #价量横截面统计特征（均值，标准差，偏度，峰度）\n",
    "    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n",
    "        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n",
    "        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n",
    "\n",
    "\n",
    "    # V3\n",
    "    # Calculate shifted and return features for specific columns\n",
    "    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n",
    "        for window in [1, 2, 3, 5, 10]:\n",
    "            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n",
    "            df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)\n",
    "\n",
    "\n",
    "    # Calculate diff features for specific columns\n",
    "    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size',\n",
    "                'wap', 'near_price', 'far_price']:#'weighted_wap','price_spread'\n",
    "        for window in [1, 2, 3, 5, 10]:\n",
    "            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n",
    "\n",
    "    #V4\n",
    "    for window in [3, 5, 10]:\n",
    "        df[f'price_change_diff_{window}'] = df[f'bid_price_diff_{window}'] - df[f'ask_price_diff_{window}']\n",
    "        df[f'size_change_diff_{window}'] = df[f'bid_size_diff_{window}'] - df[f'ask_size_diff_{window}']\n",
    "\n",
    "    #V5\n",
    "    pl_df = pl.from_pandas(df)\n",
    "\n",
    "    windows = [3, 5, 10]\n",
    "    columns = ['ask_price', 'bid_price', 'ask_size', 'bid_size']\n",
    "\n",
    "    group = [\"stock_id\"]\n",
    "    expressions = []\n",
    "\n",
    "    for window in windows:\n",
    "        for col in columns:\n",
    "            rolling_mean_expr = (\n",
    "                pl.col(f\"{col}_diff_{window}\")\n",
    "                .rolling_mean(window)\n",
    "                .over(group)\n",
    "                .alias(f'rolling_diff_{col}_{window}')\n",
    "            )\n",
    "\n",
    "            rolling_std_expr = (\n",
    "                pl.col(f\"{col}_diff_{window}\")\n",
    "                .rolling_std(window)\n",
    "                .over(group)\n",
    "                .alias(f'rolling_std_diff_{col}_{window}')\n",
    "            )\n",
    "\n",
    "            expressions.append(rolling_mean_expr)\n",
    "            expressions.append(rolling_std_expr)\n",
    "\n",
    "    lazy_df = pl_df.lazy().with_columns(expressions)\n",
    "\n",
    "    pl_df = lazy_df.collect()\n",
    "\n",
    "    df = pl_df.to_pandas()\n",
    "    gc.collect()\n",
    "\n",
    "    df['mid_price*volume'] = df['mid_price_movement'] * df['volume']\n",
    "    df['harmonic_imbalance'] = df.eval('2 / ((1 / bid_size) + (1 / ask_size))')\n",
    "\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].replace([np.inf, -np.inf], 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "# generate time & stock features\n",
    "def other_features(df):\n",
    "    #df[\"dow\"] = df[\"date_id\"] % 5\n",
    "    df[\"dom\"] = df[\"date_id\"] % 20\n",
    "    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60\n",
    "    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60\n",
    "\n",
    "    for key, value in global_stock_id_feats.items():\n",
    "        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T08:01:28.044277Z",
     "iopub.status.busy": "2023-12-20T08:01:28.043812Z",
     "iopub.status.idle": "2023-12-20T08:01:28.058373Z",
     "shell.execute_reply": "2023-12-20T08:01:28.057373Z",
     "shell.execute_reply.started": "2023-12-20T08:01:28.044239Z"
    },
    "id": "EB68X077qQyZ"
   },
   "outputs": [],
   "source": [
    "#Stock weights for calculating imbalnce features\n",
    "weights = [\n",
    "    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n",
    "    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n",
    "    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n",
    "    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n",
    "    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n",
    "    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n",
    "    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n",
    "    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n",
    "    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n",
    "    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n",
    "    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n",
    "    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n",
    "    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n",
    "    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n",
    "    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n",
    "    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n",
    "    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n",
    "]\n",
    "\n",
    "weights = {int(k):v for k,v in enumerate(weights)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOhnuy3Ey5CX"
   },
   "source": [
    "### 2.2 Stock Classification Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YymmVYonzHT9"
   },
   "source": [
    "We first calculate ***correlation matrix*** of WAP, based on which we assign clusters to stocks. 1）After normalize the clusters, ranging from （-1, 1）, we try to construct ***stock classification features***. 2）We try to apply Stratifies K-Fold Cross Validation, so to speak carrying out CV by groups to train model ( LightGBM ) and fature selection ( REF )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T06:11:15.893431Z",
     "iopub.status.busy": "2023-12-20T06:11:15.893088Z",
     "iopub.status.idle": "2023-12-20T06:14:26.932352Z",
     "shell.execute_reply": "2023-12-20T06:14:26.931545Z",
     "shell.execute_reply.started": "2023-12-20T06:11:15.893399Z"
    },
    "id": "AQq5xtFWqQyZ"
   },
   "outputs": [],
   "source": [
    "# #Correlation Matrix of WAP\n",
    "# def calculate_daily_returns(stock_data):\n",
    "#     stock_data['return'] = stock_data['wap'].pct_change()\n",
    "#     return stock_data[['return', 'seconds_in_bucket']].dropna()  # Keep 'return' and 'seconds_in_bucket' columns\n",
    "\n",
    "# returns = df.groupby(['stock_id', 'date_id']).apply(calculate_daily_returns).reset_index()\n",
    "\n",
    "# # 2. Align the data for each stock by filling in the gaps (if any) and then concatenate the returns to form a matrix\n",
    "# # For this step, we will pivot the data so each stock has its own column, and each row represents a timestamp.\n",
    "# pivot_returns = returns.pivot_table(index=['date_id', 'seconds_in_bucket'],\n",
    "#                                     columns='stock_id',\n",
    "#                                     values='return')\n",
    "\n",
    "# # handle missing values by filling the average of all available\n",
    "# pivot_returns = pivot_returns.apply(lambda row: row.fillna(row.mean()), axis=1)\n",
    "\n",
    "# # 3. Compute the correlation matrix for all stocks\n",
    "# correlation_matrix = pivot_returns.corr()\n",
    "# #print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T06:14:26.933756Z",
     "iopub.status.busy": "2023-12-20T06:14:26.933457Z",
     "iopub.status.idle": "2023-12-20T06:14:26.948356Z",
     "shell.execute_reply": "2023-12-20T06:14:26.947475Z",
     "shell.execute_reply.started": "2023-12-20T06:14:26.933731Z"
    },
    "id": "w56y-wwyqQyZ",
    "outputId": "b55dfefc-04a6-41c9-b40a-bca013b46a2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     stock_id  cluster\n",
      "0           0        4\n",
      "1           1        7\n",
      "2           2        7\n",
      "3           3        4\n",
      "4           4        4\n",
      "..        ...      ...\n",
      "195       195        5\n",
      "196       196        9\n",
      "197       197        3\n",
      "198       198        5\n",
      "199       199        9\n",
      "\n",
      "[200 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# # Hierarchical clustering\n",
    "# Z = linkage(correlation_matrix, 'ward')\n",
    "# clusters = fcluster(Z, 10, criterion = 'maxclust')\n",
    "\n",
    "# #Assign clusters to stocks\n",
    "# stock_clusters = pd.DataFrame({'stock_id': correlation_matrix.index, 'cluster': clusters})\n",
    "# print(stock_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T06:14:26.950744Z",
     "iopub.status.busy": "2023-12-20T06:14:26.950051Z",
     "iopub.status.idle": "2023-12-20T06:14:26.957102Z",
     "shell.execute_reply": "2023-12-20T06:14:26.956352Z",
     "shell.execute_reply.started": "2023-12-20T06:14:26.950712Z"
    },
    "id": "x8h88J3VqQya"
   },
   "outputs": [],
   "source": [
    "# #Normalized stock cluster feature\n",
    "\n",
    "# def cluster_feature(df):\n",
    "#     #normalize\n",
    "#     scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "#     normalized_clusters = scaler.fit_transform(clusters.reshape(-1, 1))\n",
    "\n",
    "#     # Assign clusters to stocks\n",
    "\n",
    "#     # Assign clusters to stocks\n",
    "#     # 1. Extract cluster labels from hierarchical clustering\n",
    "#     df_clusters = pd.DataFrame({'stock_id': pivot_returns.columns, 'cluster_label': normalized_clusters.flatten()})\n",
    "\n",
    "#     # 2. Map cluster labels to each stock ID\n",
    "#     stock_id_to_cluster = dict(zip(df_clusters['stock_id'], df_clusters['cluster_label']))\n",
    "\n",
    "#     # 3. Add cluster labels to your original DataFrame\n",
    "#     df['cluster'] = df['stock_id'].map(stock_id_to_cluster)\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFhYZcZ10qn0"
   },
   "source": [
    "### 2.3 PCA_Weighed Average Features * 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8AXF3Wf0wsO"
   },
   "source": [
    "In this section, we apply PCA on WAG and retrieve the first four elements as features for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T06:14:26.958406Z",
     "iopub.status.busy": "2023-12-20T06:14:26.958068Z",
     "iopub.status.idle": "2023-12-20T06:14:26.967705Z",
     "shell.execute_reply": "2023-12-20T06:14:26.966906Z",
     "shell.execute_reply.started": "2023-12-20T06:14:26.958375Z"
    },
    "id": "iU8zwMqGqQya"
   },
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# pca = PCA()\n",
    "# principal_components = pca.fit_transform(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T06:14:26.968948Z",
     "iopub.status.busy": "2023-12-20T06:14:26.968679Z",
     "iopub.status.idle": "2023-12-20T06:14:26.981699Z",
     "shell.execute_reply": "2023-12-20T06:14:26.981001Z",
     "shell.execute_reply.started": "2023-12-20T06:14:26.968925Z"
    },
    "id": "agZC68IhqQya"
   },
   "outputs": [],
   "source": [
    "# #PCA-weighed average price features\n",
    "# def pca_wap_feature(df):\n",
    "\n",
    "#     #Focus on the first 4 components and save as DataFrame\n",
    "\n",
    "#     #Create a pivot table for wap\n",
    "#     price_pivot = df.pivot_table(index=['date_id', 'seconds_in_bucket'], columns='stock_id', values='wap')\n",
    "\n",
    "#     #Generate principal DataFrame\n",
    "#     principal_df = pd.DataFrame(data=principal_components,\n",
    "#                                 index=correlation_matrix.index,  # use stock_ids as the index\n",
    "#                                 columns=['PC'+str(i) for i in range(1, principal_components.shape[1] + 1)])\n",
    "\n",
    "#     #Ensure the ordering of stock_id in price_pivot and principal_pca is consistent\n",
    "#     ordered_columns = price_pivot.columns\n",
    "#     principal_df = principal_df.loc[ordered_columns].reset_index()\n",
    "\n",
    "#     #Handle NaN values and replace with 0\n",
    "#     price_pivot.fillna(0, inplace=True)\n",
    "#     principal_df.fillna(0, inplace=True)\n",
    "\n",
    "#     #Initialize a dataframe to hold the PCA_WAP values\n",
    "#     pca_wap_df = pd.DataFrame(index=price_pivot.index)\n",
    "\n",
    "#     #Compute 4 WAPs using PCA\n",
    "#     for i in range(1,5):\n",
    "#         pca_wap_df[f'PCA_WAP_{i}'] = (price_pivot.values * principal_df.set_index('stock_id')[f'PC{i}'].values).sum(axis=1)\n",
    "\n",
    "#     #Resetting index for merging purposes\n",
    "#     pca_wap_df = pca_wap_df.reset_index()\n",
    "\n",
    "#     #Merging the PCA_WAP columns with the initial dataset df\n",
    "#     df = df.merge(pca_wap_df, on=['date_id', 'seconds_in_bucket'], how='left')\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNXjHo4n1PyZ"
   },
   "source": [
    "### 2.4 Feature Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T08:02:40.888601Z",
     "iopub.status.busy": "2023-12-20T08:02:40.887852Z",
     "iopub.status.idle": "2023-12-20T08:02:40.894397Z",
     "shell.execute_reply": "2023-12-20T08:02:40.893408Z",
     "shell.execute_reply.started": "2023-12-20T08:02:40.888571Z"
    },
    "id": "_sMo3LtAqQya"
   },
   "outputs": [],
   "source": [
    "# generate all features\n",
    "def generate_all_features(df):\n",
    "    cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\", \"target\"]]\n",
    "    df = df[cols]\n",
    "    df = imbalance_features(df)\n",
    "    df = other_features(df)\n",
    "    #df = cluster_feature(df)\n",
    "    #df = pca_wap_feature(df)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    feature_name = [i for i in df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\"]]\n",
    "\n",
    "    return df[feature_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMtZod5f1US0"
   },
   "source": [
    "## 3 Feature Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6H_L5qiS3Fka"
   },
   "source": [
    "### 3.1 Dataset Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hU222Qwg1Yam"
   },
   "source": [
    "When the code is running offline, the entire dataset will be used for training. When the code is running online, 90% data will be used for training and the rest will be used for validation.\n",
    "Additionally, for memory saving, after data splitting, we delete the original dataset, so to speak, dataframe, ***df***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T08:02:47.702311Z",
     "iopub.status.busy": "2023-12-20T08:02:47.701942Z",
     "iopub.status.idle": "2023-12-20T08:02:48.248760Z",
     "shell.execute_reply": "2023-12-20T08:02:48.247825Z",
     "shell.execute_reply.started": "2023-12-20T08:02:47.702284Z"
    },
    "id": "qq2Vi6Y8qQya",
    "outputId": "4b681868-d773-41e1-89b6-3c7833dd7a8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offline mode\n",
      "train : (4742893, 17), valid : (494999, 17)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the code is running in offline or online mode\n",
    "if is_offline:\n",
    "    # In offline mode, split the data into training and validation sets based on the split_day\n",
    "    df_train = df[df[\"date_id\"] <= split_day]\n",
    "    df_valid = df[df[\"date_id\"] > split_day]\n",
    "    print(\"Offline mode\")\n",
    "    print(f\"train : {df_train.shape}, valid : {df_valid.shape}\")\n",
    "else:\n",
    "    # In online mode, use the entire dataset for training\n",
    "    df_train = df\n",
    "    print(\"Online mode\")\n",
    "\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6vctrXf3Iuv"
   },
   "source": [
    "### 3.2 Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T08:06:57.065328Z",
     "iopub.status.busy": "2023-12-20T08:06:57.064539Z",
     "iopub.status.idle": "2023-12-20T08:08:25.816323Z",
     "shell.execute_reply": "2023-12-20T08:08:25.815316Z",
     "shell.execute_reply.started": "2023-12-20T08:06:57.065294Z"
    },
    "id": "mxYW6TGBqQya",
    "outputId": "0368edc2-231d-45c1-a8b7-9139c96829b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build Train Feats Finished.\n",
      "Build Valid Feats Finished.\n",
      "Feature length = 173\n",
      "CPU times: user 1min 20s, sys: 19 s, total: 1min 39s\n",
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if is_train:\n",
    "    global_stock_id_feats = {\n",
    "        \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median()\n",
    "        #\"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
    "        #\"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
    "        #\"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n",
    "        #\"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
    "        #\"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
    "    }\n",
    "    if is_offline:\n",
    "        df_train_feats = generate_all_features(df_train)\n",
    "        print(\"Build Train Feats Finished.\")\n",
    "        df_valid_feats = generate_all_features(df_valid)\n",
    "        print(\"Build Valid Feats Finished.\")\n",
    "        df_valid_feats = reduce_mem_usage(df_valid_feats)\n",
    "    else:\n",
    "        df_train_feats = generate_all_features(df_train)\n",
    "        print(\"Build Online Train Feats Finished.\")\n",
    "\n",
    "    df_train_feats = reduce_mem_usage(df_train_feats)\n",
    "\n",
    "feature_name = list(df_train_feats.columns)\n",
    "print(f\"Feature length = {len(feature_name)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQ12CZd03ey4"
   },
   "source": [
    "## 4 Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUWIFSee3g1m"
   },
   "source": [
    "We apply LightGBM to train the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T08:09:34.195344Z",
     "iopub.status.busy": "2023-12-20T08:09:34.194444Z",
     "iopub.status.idle": "2023-12-20T08:09:34.200196Z",
     "shell.execute_reply": "2023-12-20T08:09:34.199288Z",
     "shell.execute_reply.started": "2023-12-20T08:09:34.195310Z"
    }
   },
   "outputs": [],
   "source": [
    "del_list = [\n",
    "     #'PCA_WAP_3',\n",
    "     #'global_std_price',\n",
    "     #'PCA_WAP_2',\n",
    "     #'dow',\n",
    "     'near_price_wap_imb',\n",
    "     'weighted_wap',\n",
    "     'near_price_diff_10',\n",
    "     'ask_price_wap_imb',\n",
    "     #'PCA_WAP_4',\n",
    "     'reference_price_near_price_imb',\n",
    "     #'cluster',\n",
    "     #'global_std_size',\n",
    "     #'PCA_WAP_1',\n",
    "     'near_price_diff_1',\n",
    "     #'global_median_price',\n",
    "     'near_price',\n",
    "     'reference_price_far_price_imb',\n",
    "     #'global_ptp_size',\n",
    "     'all_prices_mean',\n",
    "     #'global_ptp_price',\n",
    "     'all_prices_skew',\n",
    "     'all_prices_kurt',\n",
    "     'ask_price_bid_price_reference_price_imb2',\n",
    "     'wap_diff_10'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T08:09:36.396378Z",
     "iopub.status.busy": "2023-12-20T08:09:36.395963Z",
     "iopub.status.idle": "2023-12-20T08:09:38.913594Z",
     "shell.execute_reply": "2023-12-20T08:09:38.912656Z",
     "shell.execute_reply.started": "2023-12-20T08:09:36.396350Z"
    },
    "id": "vmc5Yi4TTFab",
    "outputId": "8c3aa5ac-3c1c-4d87-f8f1-f5eefe83ba7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature length = 160\n"
     ]
    }
   ],
   "source": [
    "if is_offline:\n",
    "    df_train_feats.drop(del_list,axis=1,inplace=True)\n",
    "    df_valid_feats.drop(del_list,axis=1,inplace=True)\n",
    "else:\n",
    "    df_train_feats.drop(del_list,axis=1,inplace=True)\n",
    "feature_name = list(df_train_feats.columns)\n",
    "print(f\"Feature length = {len(feature_name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T08:09:45.401686Z",
     "iopub.status.busy": "2023-12-20T08:09:45.401354Z",
     "iopub.status.idle": "2023-12-20T09:57:22.584489Z",
     "shell.execute_reply": "2023-12-20T09:57:22.583714Z",
     "shell.execute_reply.started": "2023-12-20T08:09:45.401660Z"
    },
    "id": "jXVtNDMbEcYQ",
    "outputId": "f85fedbe-4fe8-4dc4-d1db-884510333178"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature length = 160\n",
      "Fold 1 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 5.47129\n",
      "[200]\tvalid_0's l1: 5.44211\n",
      "[300]\tvalid_0's l1: 5.43037\n",
      "[400]\tvalid_0's l1: 5.42256\n",
      "[500]\tvalid_0's l1: 5.41803\n",
      "[600]\tvalid_0's l1: 5.41478\n",
      "[700]\tvalid_0's l1: 5.41253\n",
      "[800]\tvalid_0's l1: 5.41083\n",
      "[900]\tvalid_0's l1: 5.40965\n",
      "[1000]\tvalid_0's l1: 5.40867\n",
      "[1100]\tvalid_0's l1: 5.40798\n",
      "[1200]\tvalid_0's l1: 5.40741\n",
      "[1300]\tvalid_0's l1: 5.40692\n",
      "[1400]\tvalid_0's l1: 5.40636\n",
      "[1500]\tvalid_0's l1: 5.40609\n",
      "[1600]\tvalid_0's l1: 5.40591\n",
      "[1700]\tvalid_0's l1: 5.40548\n",
      "[1800]\tvalid_0's l1: 5.40536\n",
      "[1900]\tvalid_0's l1: 5.40523\n",
      "[2000]\tvalid_0's l1: 5.40507\n",
      "[2100]\tvalid_0's l1: 5.40498\n",
      "[2200]\tvalid_0's l1: 5.40483\n",
      "[2300]\tvalid_0's l1: 5.40479\n",
      "Early stopping, best iteration is:\n",
      "[2287]\tvalid_0's l1: 5.40477\n",
      "Model for fold 1 saved to modelitos_para_despues/doblez_1.txt\n",
      "Fold 1 MAE: 5.4047657952458295\n",
      "Fold 2 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 7.1926\n",
      "[200]\tvalid_0's l1: 7.15212\n",
      "[300]\tvalid_0's l1: 7.1347\n",
      "[400]\tvalid_0's l1: 7.12408\n",
      "[500]\tvalid_0's l1: 7.11738\n",
      "[600]\tvalid_0's l1: 7.1123\n",
      "[700]\tvalid_0's l1: 7.10882\n",
      "[800]\tvalid_0's l1: 7.10611\n",
      "[900]\tvalid_0's l1: 7.10391\n",
      "[1000]\tvalid_0's l1: 7.10253\n",
      "[1100]\tvalid_0's l1: 7.1014\n",
      "[1200]\tvalid_0's l1: 7.10038\n",
      "[1300]\tvalid_0's l1: 7.09956\n",
      "[1400]\tvalid_0's l1: 7.09905\n",
      "[1500]\tvalid_0's l1: 7.09872\n",
      "[1600]\tvalid_0's l1: 7.09816\n",
      "[1700]\tvalid_0's l1: 7.09777\n",
      "[1800]\tvalid_0's l1: 7.09747\n",
      "[1900]\tvalid_0's l1: 7.09706\n",
      "[2000]\tvalid_0's l1: 7.09678\n",
      "[2100]\tvalid_0's l1: 7.09659\n",
      "[2200]\tvalid_0's l1: 7.09644\n",
      "[2300]\tvalid_0's l1: 7.09613\n",
      "[2400]\tvalid_0's l1: 7.09586\n",
      "[2500]\tvalid_0's l1: 7.0955\n",
      "[2600]\tvalid_0's l1: 7.09526\n",
      "[2700]\tvalid_0's l1: 7.09503\n",
      "[2800]\tvalid_0's l1: 7.09481\n",
      "[2900]\tvalid_0's l1: 7.0947\n",
      "[3000]\tvalid_0's l1: 7.09454\n",
      "[3100]\tvalid_0's l1: 7.09443\n",
      "[3200]\tvalid_0's l1: 7.0943\n",
      "[3300]\tvalid_0's l1: 7.09411\n",
      "[3400]\tvalid_0's l1: 7.09397\n",
      "[3500]\tvalid_0's l1: 7.09391\n",
      "[3600]\tvalid_0's l1: 7.09381\n",
      "[3700]\tvalid_0's l1: 7.09373\n",
      "[3800]\tvalid_0's l1: 7.09362\n",
      "[3900]\tvalid_0's l1: 7.09359\n",
      "Early stopping, best iteration is:\n",
      "[3862]\tvalid_0's l1: 7.09358\n",
      "Model for fold 2 saved to modelitos_para_despues/doblez_2.txt\n",
      "Fold 2 MAE: 7.093575950420649\n",
      "Fold 3 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 6.75524\n",
      "[200]\tvalid_0's l1: 6.72614\n",
      "[300]\tvalid_0's l1: 6.71348\n",
      "[400]\tvalid_0's l1: 6.70495\n",
      "[500]\tvalid_0's l1: 6.69938\n",
      "[600]\tvalid_0's l1: 6.69558\n",
      "[700]\tvalid_0's l1: 6.69296\n",
      "[800]\tvalid_0's l1: 6.6908\n",
      "[900]\tvalid_0's l1: 6.68911\n",
      "[1000]\tvalid_0's l1: 6.68783\n",
      "[1100]\tvalid_0's l1: 6.68698\n",
      "[1200]\tvalid_0's l1: 6.68628\n",
      "[1300]\tvalid_0's l1: 6.68584\n",
      "[1400]\tvalid_0's l1: 6.68551\n",
      "[1500]\tvalid_0's l1: 6.68524\n",
      "[1600]\tvalid_0's l1: 6.68508\n",
      "[1700]\tvalid_0's l1: 6.68477\n",
      "[1800]\tvalid_0's l1: 6.68449\n",
      "[1900]\tvalid_0's l1: 6.68412\n",
      "[2000]\tvalid_0's l1: 6.68378\n",
      "[2100]\tvalid_0's l1: 6.68364\n",
      "[2200]\tvalid_0's l1: 6.68352\n",
      "[2300]\tvalid_0's l1: 6.68336\n",
      "[2400]\tvalid_0's l1: 6.68327\n",
      "[2500]\tvalid_0's l1: 6.6831\n",
      "[2600]\tvalid_0's l1: 6.68291\n",
      "[2700]\tvalid_0's l1: 6.68282\n",
      "[2800]\tvalid_0's l1: 6.68271\n",
      "[2900]\tvalid_0's l1: 6.68253\n",
      "[3000]\tvalid_0's l1: 6.68244\n",
      "[3100]\tvalid_0's l1: 6.68231\n",
      "[3200]\tvalid_0's l1: 6.68223\n",
      "[3300]\tvalid_0's l1: 6.68211\n",
      "[3400]\tvalid_0's l1: 6.68195\n",
      "[3500]\tvalid_0's l1: 6.68191\n",
      "[3600]\tvalid_0's l1: 6.68186\n",
      "[3700]\tvalid_0's l1: 6.68187\n",
      "Early stopping, best iteration is:\n",
      "[3630]\tvalid_0's l1: 6.68183\n",
      "Model for fold 3 saved to modelitos_para_despues/doblez_3.txt\n",
      "Fold 3 MAE: 6.681833429898666\n",
      "Fold 4 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 6.23977\n",
      "[200]\tvalid_0's l1: 6.21619\n",
      "[300]\tvalid_0's l1: 6.20628\n",
      "[400]\tvalid_0's l1: 6.19967\n",
      "[500]\tvalid_0's l1: 6.1955\n",
      "[600]\tvalid_0's l1: 6.19301\n",
      "[700]\tvalid_0's l1: 6.19086\n",
      "[800]\tvalid_0's l1: 6.1895\n",
      "[900]\tvalid_0's l1: 6.18851\n",
      "[1000]\tvalid_0's l1: 6.18771\n",
      "[1100]\tvalid_0's l1: 6.18706\n",
      "[1200]\tvalid_0's l1: 6.18646\n",
      "[1300]\tvalid_0's l1: 6.18603\n",
      "[1400]\tvalid_0's l1: 6.18565\n",
      "[1500]\tvalid_0's l1: 6.18545\n",
      "[1600]\tvalid_0's l1: 6.18539\n",
      "[1700]\tvalid_0's l1: 6.18524\n",
      "[1800]\tvalid_0's l1: 6.18511\n",
      "[1900]\tvalid_0's l1: 6.18492\n",
      "[2000]\tvalid_0's l1: 6.18479\n",
      "[2100]\tvalid_0's l1: 6.1847\n",
      "[2200]\tvalid_0's l1: 6.18454\n",
      "[2300]\tvalid_0's l1: 6.1845\n",
      "[2400]\tvalid_0's l1: 6.18442\n",
      "[2500]\tvalid_0's l1: 6.18439\n",
      "[2600]\tvalid_0's l1: 6.18434\n",
      "[2700]\tvalid_0's l1: 6.18427\n",
      "Early stopping, best iteration is:\n",
      "[2672]\tvalid_0's l1: 6.18425\n",
      "Model for fold 4 saved to modelitos_para_despues/doblez_4.txt\n",
      "Fold 4 MAE: 6.184250194695441\n",
      "Fold 5 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 6.1395\n",
      "[200]\tvalid_0's l1: 6.116\n",
      "[300]\tvalid_0's l1: 6.10586\n",
      "[400]\tvalid_0's l1: 6.09867\n",
      "[500]\tvalid_0's l1: 6.0942\n",
      "[600]\tvalid_0's l1: 6.09137\n",
      "[700]\tvalid_0's l1: 6.0891\n",
      "[800]\tvalid_0's l1: 6.08739\n",
      "[900]\tvalid_0's l1: 6.08629\n",
      "[1000]\tvalid_0's l1: 6.08552\n",
      "[1100]\tvalid_0's l1: 6.08491\n",
      "[1200]\tvalid_0's l1: 6.08433\n",
      "[1300]\tvalid_0's l1: 6.08397\n",
      "[1400]\tvalid_0's l1: 6.08374\n",
      "[1500]\tvalid_0's l1: 6.08358\n",
      "[1600]\tvalid_0's l1: 6.08345\n",
      "[1700]\tvalid_0's l1: 6.08323\n",
      "Early stopping, best iteration is:\n",
      "[1689]\tvalid_0's l1: 6.08321\n",
      "Model for fold 5 saved to modelitos_para_despues/doblez_5.txt\n",
      "Fold 5 MAE: 6.083207074398709\n",
      "Training final model with average best iteration: 2828\n",
      "Final model saved to modelitos_para_despues/doblez-conjunto.txt\n",
      "Average MAE across all folds: 6.28952648893186\n",
      "Offline Score 5.8244\n"
     ]
    }
   ],
   "source": [
    "# Set LightGBM parameters\n",
    "lgb_params = {\n",
    "    \"objective\": \"mae\",\n",
    "    \"n_estimators\": 5000, #less estimators\n",
    "    \"num_leaves\": 256, #more leaves\n",
    "    \"subsample\": 0.6,\n",
    "    \"colsample_bytree\": 0.6,\n",
    "    \"learning_rate\": 0.00871, #larger learning rate(from 0.00871 to 0.00005)\n",
    "    'max_depth': 11,\n",
    "    \"n_jobs\": 4,\n",
    "    \"device\": \"gpu\",\n",
    "    \"verbosity\": -1,\n",
    "    \"importance_type\": \"gain\",\n",
    "}\n",
    "# Get feature names\n",
    "feature_name = list(df_train_feats.columns)\n",
    "print(f\"Feature length = {len(feature_name)}\")\n",
    "\n",
    "# Set up cross-validation parameters\n",
    "num_folds = 5\n",
    "fold_size = 480 // num_folds #online mode\n",
    "gap = 5\n",
    "\n",
    "# Initialize lists to store models and scores\n",
    "models = []\n",
    "scores = []\n",
    "\n",
    "# Set model save path\n",
    "model_save_path = 'modelitos_para_despues'\n",
    "if not os.path.exists(model_save_path):\n",
    "    os.makedirs(model_save_path)\n",
    "\n",
    "# Get date IDs from the training data\n",
    "date_ids = df_train['date_id'].values\n",
    "\n",
    "# Loop over folds for cross-validation\n",
    "for i in range(num_folds):\n",
    "    start = i * fold_size\n",
    "    end = start + fold_size\n",
    "    \n",
    "    # Define the purged set ranges\n",
    "    purged_before_start = start - 2\n",
    "    purged_before_end = start + 2\n",
    "    purged_after_start = end - 2\n",
    "    purged_after_end = end + 2\n",
    "    \n",
    "    # Exclude the purged ranges from the test set\n",
    "    purged_set = ((date_ids >= purged_before_start) & (date_ids <= purged_before_end)) | \\\n",
    "                 ((date_ids >= purged_after_start) & (date_ids <= purged_after_end))\n",
    "    \n",
    "    # Define test_indices excluding the purged set\n",
    "    test_indices = (date_ids >= start) & (date_ids < end) & ~purged_set\n",
    "    train_indices = ~test_indices & ~purged_set\n",
    "\n",
    "    # Create fold-specific training and validation sets\n",
    "    df_fold_train = df_train_feats[train_indices]\n",
    "    df_fold_train_target = df_train['target'][train_indices]\n",
    "    df_fold_valid = df_train_feats[test_indices]\n",
    "    df_fold_valid_target = df_train['target'][test_indices]\n",
    "\n",
    "    print(f\"Fold {i+1} Model Training\")\n",
    "\n",
    "    # Train a LightGBM model for the current fold\n",
    "    lgb_model = lgb.LGBMRegressor(**lgb_params)\n",
    "    lgb_model.fit(\n",
    "        df_fold_train[feature_name],\n",
    "        df_fold_train_target,\n",
    "        eval_set=[(df_fold_valid[feature_name], df_fold_valid_target)],\n",
    "        callbacks=[\n",
    "            lgb.callback.early_stopping(stopping_rounds=100),\n",
    "            lgb.callback.log_evaluation(period=100),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    models.append(lgb_model)\n",
    "\n",
    "    # Save the model to a file\n",
    "    model_filename = os.path.join(model_save_path, f'doblez_{i+1}.txt')\n",
    "    lgb_model.booster_.save_model(model_filename)\n",
    "    print(f\"Model for fold {i+1} saved to {model_filename}\")\n",
    "\n",
    "    # Evaluate model performance on the validation set\n",
    "    fold_predictions = lgb_model.predict(df_fold_valid[feature_name])\n",
    "    fold_score = mean_absolute_error(fold_predictions, df_fold_valid_target)\n",
    "    scores.append(fold_score)\n",
    "    print(f\"Fold {i+1} MAE: {fold_score}\")\n",
    "\n",
    "    # Free up memory by deleting fold-specific variables\n",
    "    del df_fold_train, df_fold_train_target, df_fold_valid, df_fold_valid_target\n",
    "    gc.collect()\n",
    "\n",
    "# Calculate the average best iteration from all regular folds\n",
    "average_best_iteration = int(np.mean([model.best_iteration_ for model in models]))\n",
    "\n",
    "# Update the lgb_params with the average best iteration\n",
    "final_model_params = lgb_params.copy()\n",
    "final_model_params['n_estimators'] = average_best_iteration\n",
    "\n",
    "print(f\"Training final model with average best iteration: {average_best_iteration}\")\n",
    "\n",
    "# Train the final model on the entire dataset\n",
    "final_model = lgb.LGBMRegressor(**final_model_params)\n",
    "final_model.fit(\n",
    "    df_train_feats[feature_name],\n",
    "    df_train['target'],\n",
    "    callbacks=[\n",
    "        lgb.callback.log_evaluation(period=100),\n",
    "    ],\n",
    ")\n",
    "# Append the final model to the list of models\n",
    "models.append(final_model)\n",
    "\n",
    "# Append the final model to the list of models\n",
    "models.append(final_model)\n",
    "\n",
    "# Save the final model to a file\n",
    "final_model_filename = os.path.join(model_save_path, 'doblez-conjunto.txt')\n",
    "final_model.booster_.save_model(final_model_filename)\n",
    "print(f\"Final model saved to {final_model_filename}\")\n",
    "\n",
    "# Now 'models' holds the trained models for each fold and 'scores' holds the validation scores\n",
    "print(f\"Average MAE across all folds: {np.mean(scores)}\")\n",
    "\n",
    "if is_offline:\n",
    "    # offline predictions\n",
    "    df_valid_target = df_valid[\"target\"]\n",
    "    offline_predictions = final_model.predict(df_valid_feats[feature_name])\n",
    "    offline_score = mean_absolute_error(offline_predictions, df_valid_target)\n",
    "    print(f\"Offline Score {np.round(offline_score, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T09:58:10.548141Z",
     "iopub.status.busy": "2023-12-20T09:58:10.547447Z",
     "iopub.status.idle": "2023-12-20T09:58:10.553652Z",
     "shell.execute_reply": "2023-12-20T09:58:10.552637Z",
     "shell.execute_reply.started": "2023-12-20T09:58:10.548107Z"
    }
   },
   "outputs": [],
   "source": [
    "def weighted_average(a):\n",
    "    w = []\n",
    "    n = len(a)\n",
    "    for j in range(1, n + 1):\n",
    "        j = 2 if j == 1 else j\n",
    "        w.append(1 / (2**(n + 1 - j)))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T09:59:03.526263Z",
     "iopub.status.busy": "2023-12-20T09:59:03.525884Z",
     "iopub.status.idle": "2023-12-20T10:03:03.333519Z",
     "shell.execute_reply": "2023-12-20T10:03:03.332523Z",
     "shell.execute_reply.started": "2023-12-20T09:59:03.526235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n",
      "10 qps: 1.4992877960205078\n",
      "20 qps: 1.460757076740265\n",
      "30 qps: 1.453003184000651\n",
      "40 qps: 1.4526222884654998\n",
      "50 qps: 1.445951738357544\n",
      "60 qps: 1.4458695769309997\n",
      "70 qps: 1.4435142959867204\n",
      "80 qps: 1.4434219866991043\n",
      "90 qps: 1.4424589978324043\n",
      "100 qps: 1.440963122844696\n",
      "110 qps: 1.4403594623912463\n",
      "120 qps: 1.4443520585695901\n",
      "130 qps: 1.4443986012385441\n",
      "140 qps: 1.4461432780538286\n",
      "150 qps: 1.4472796964645385\n",
      "160 qps: 1.4471724301576614\n",
      "The code will take approximately 1.6592 hours to reason about\n"
     ]
    }
   ],
   "source": [
    "def zero_sum(prices, volumes):\n",
    "    std_error = np.sqrt(volumes)\n",
    "    step = np.sum(prices) / np.sum(std_error)\n",
    "    out = prices - std_error * step\n",
    "    return out\n",
    "\n",
    "if is_infer:\n",
    "    import optiver2023\n",
    "    env = optiver2023.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "    counter = 0\n",
    "    y_min, y_max = -64, 64\n",
    "    qps, predictions = [], []\n",
    "    cache = pd.DataFrame()\n",
    "\n",
    "    # Weights for each fold model\n",
    "    model_weights = weighted_average(models)\n",
    "    \n",
    "    for (test, revealed_targets, sample_prediction) in iter_test:\n",
    "        now_time = time.time()\n",
    "        cache = pd.concat([cache, test], ignore_index=True, axis=0)\n",
    "        if counter > 0:\n",
    "            cache = cache.groupby(['stock_id']).tail(21).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n",
    "        feat = generate_all_features(cache)[-len(test):]\n",
    "        feat.drop(del_list,axis=1,inplace=True)\n",
    "\n",
    "        # Generate predictions for each model and calculate the weighted average\n",
    "        lgb_predictions = np.zeros(len(test))\n",
    "        for model, weight in zip(models, model_weights):\n",
    "            lgb_predictions += weight * model.predict(feat[feature_name])\n",
    "\n",
    "        lgb_predictions = zero_sum(lgb_predictions, test['bid_size'] + test['ask_size'])\n",
    "        clipped_predictions = np.clip(lgb_predictions, y_min, y_max)\n",
    "        sample_prediction['target'] = clipped_predictions\n",
    "        env.predict(sample_prediction)\n",
    "        counter += 1\n",
    "        qps.append(time.time() - now_time)\n",
    "        if counter % 10 == 0:\n",
    "            print(counter, 'qps:', np.mean(qps))\n",
    "\n",
    "    time_cost = 1.146 * np.mean(qps)\n",
    "    print(f\"The code will take approximately {np.round(time_cost, 4)} hours to reason about\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7056235,
     "sourceId": 57891,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30627,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
