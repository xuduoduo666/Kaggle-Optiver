{"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V100"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":57891,"databundleVersionId":7056235,"sourceType":"competition"}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Version 1: Feature Selection","metadata":{"id":"Rf2V9fkbxmr9"}},{"cell_type":"markdown","source":"## 1 Download Data","metadata":{"id":"fDmPBpCLx3a4"}},{"cell_type":"markdown","source":"## 2 Feature Selection","metadata":{"id":"XuNqRdKhyBzr"}},{"cell_type":"code","source":"import gc\nimport os\nimport time\nimport warnings\nfrom itertools import combinations\nfrom warnings import simplefilter\n\nimport joblib\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold, TimeSeriesSplit\nimport polars as pl\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy.cluster.hierarchy import dendrogram, linkage, fcluster\nwarnings.filterwarnings(\"ignore\")\nsimplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom catboost import CatBoostRegressor, EShapCalcType, EFeaturesSelectionAlgorithm\nfrom sklearn.metrics import mean_absolute_error\n\nis_offline = False\nis_train = True\nis_infer = True\nmax_lookback = np.nan\nsplit_day = 435 #The testing data constixtute 90% = 435 / 481","metadata":{"id":"aDfQwqZCqQyW","execution":{"iopub.status.busy":"2023-12-20T06:10:51.880167Z","iopub.execute_input":"2023-12-20T06:10:51.880862Z","iopub.status.idle":"2023-12-20T06:10:56.918471Z","shell.execute_reply.started":"2023-12-20T06:10:51.880824Z","shell.execute_reply":"2023-12-20T06:10:56.917694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\")\ndf = df.dropna(subset=[\"target\"])\ndf.reset_index(drop=True, inplace=True)\ndf.shape","metadata":{"id":"GlA4mURlqQyX","outputId":"29a0fa28-c497-458d-a721-03eecae76d32","execution":{"iopub.status.busy":"2023-12-20T06:10:56.920200Z","iopub.execute_input":"2023-12-20T06:10:56.920489Z","iopub.status.idle":"2023-12-20T06:11:15.239078Z","shell.execute_reply.started":"2023-12-20T06:10:56.920464Z","shell.execute_reply":"2023-12-20T06:11:15.238182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#To reduce memory usage\ndef reduce_mem_usage(df, verbose=0):\n    \"\"\"\n    Iterate through all numeric columns of a dataframe and modify the data type\n    to reduce memory usage.\n    \"\"\"\n\n    start_mem = df.memory_usage().sum() / 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float32)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float32)\n\n    if verbose:\n        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n        end_mem = df.memory_usage().sum() / 1024**2\n        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n        decrease = 100 * (start_mem - end_mem) / start_mem\n        logger.info(f\"Decreased by {decrease:.2f}%\")\n\n    return df\n","metadata":{"id":"XSvq5VkXqQyY","execution":{"iopub.status.busy":"2023-12-20T06:11:15.240314Z","iopub.execute_input":"2023-12-20T06:11:15.240674Z","iopub.status.idle":"2023-12-20T06:11:15.253941Z","shell.execute_reply.started":"2023-12-20T06:11:15.240641Z","shell.execute_reply":"2023-12-20T06:11:15.253081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.1 Imbalance Features","metadata":{"id":"vVAgjxFVyqW7"}},{"cell_type":"code","source":"# Import Numba for just-in-time (JIT) compilation and parallel processing\nfrom numba import njit, prange\n\n# Function to compute triplet imbalance in parallel using Numba\n@njit(parallel=True)\ndef compute_triplet_imbalance(df_values, comb_indices):\n    num_rows = df_values.shape[0]\n    num_combinations = len(comb_indices)\n    imbalance_features = np.empty((num_rows, num_combinations))\n\n    # Loop through all combinations of triplets\n    for i in prange(num_combinations):\n        a, b, c = comb_indices[i]\n\n        # Loop through rows of the DataFrame\n        for j in range(num_rows):\n            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n\n            # Prevent division by zero\n            if mid_val == min_val:\n                imbalance_features[j, i] = np.nan\n            else:\n                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n\n    return imbalance_features\n\n# Function to calculate triplet imbalance for given price data and a DataFrame\ndef calculate_triplet_imbalance_numba(price, df):\n    # Convert DataFrame to numpy array for Numba compatibility\n    df_values = df[price].values\n    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n\n    # Calculate the triplet imbalance using the Numba-optimized function\n    features_array = compute_triplet_imbalance(df_values, comb_indices)\n\n    # Create a DataFrame from the results\n    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n    features = pd.DataFrame(features_array, columns=columns)\n\n    return features\n\n","metadata":{"id":"jFI4WK9iqQyY","execution":{"iopub.status.busy":"2023-12-20T06:11:15.255153Z","iopub.execute_input":"2023-12-20T06:11:15.255487Z","iopub.status.idle":"2023-12-20T06:11:15.848925Z","shell.execute_reply.started":"2023-12-20T06:11:15.255457Z","shell.execute_reply":"2023-12-20T06:11:15.848172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate imbalance features\ndef imbalance_features(df):\n    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n\n    # V1\n    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n    df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n    # Create features for pairwise price imbalances\n    for c in combinations(prices, 2):\n        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n    # Calculate triplet imbalance features using the Numba-optimized function\n    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n        df[triplet_feature.columns] = triplet_feature.values\n\n\n    # V2\n    df[\"stock_weights\"] = df[\"stock_id\"].map(weights)\n    df[\"weighted_wap\"] = df[\"stock_weights\"] * df[\"wap\"]\n    df['wap_momentum'] = df.groupby('stock_id')['weighted_wap'].pct_change(periods=6)\n    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n    df['spread_depth_ratio'] = (df['ask_price'] - df['bid_price']) / (df['bid_size'] + df['ask_size'])\n    df['mid_price_movement'] = df['mid_price'].diff(periods=5).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n    df['micro_price'] = ((df['bid_price'] * df['ask_size']) + (df['ask_price'] * df['bid_size'])) / (df['bid_size'] + df['ask_size'])\n    df['relative_spread'] = (df['ask_price'] - df['bid_price']) / df['wap']\n\n    #价量横截面统计特征（均值，标准差，偏度，峰度）\n    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n\n\n    # V3\n    # Calculate shifted and return features for specific columns\n    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n        for window in [1, 2, 3, 5, 10]:\n            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n            df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)\n\n\n    # Calculate diff features for specific columns\n    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size',\n                'wap', 'near_price', 'far_price']:#'weighted_wap','price_spread'\n        for window in [1, 2, 3, 5, 10]:\n            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n\n    #V4\n    for window in [3, 5, 10]:\n        df[f'price_change_diff_{window}'] = df[f'bid_price_diff_{window}'] - df[f'ask_price_diff_{window}']\n        df[f'size_change_diff_{window}'] = df[f'bid_size_diff_{window}'] - df[f'ask_size_diff_{window}']\n\n    #V5\n    pl_df = pl.from_pandas(df)\n\n    windows = [3, 5, 10]\n    columns = ['ask_price', 'bid_price', 'ask_size', 'bid_size']\n\n    group = [\"stock_id\"]\n    expressions = []\n\n    for window in windows:\n        for col in columns:\n            rolling_mean_expr = (\n                pl.col(f\"{col}_diff_{window}\")\n                .rolling_mean(window)\n                .over(group)\n                .alias(f'rolling_diff_{col}_{window}')\n            )\n\n            rolling_std_expr = (\n                pl.col(f\"{col}_diff_{window}\")\n                .rolling_std(window)\n                .over(group)\n                .alias(f'rolling_std_diff_{col}_{window}')\n            )\n\n            expressions.append(rolling_mean_expr)\n            expressions.append(rolling_std_expr)\n\n    lazy_df = pl_df.lazy().with_columns(expressions)\n\n    pl_df = lazy_df.collect()\n\n    df = pl_df.to_pandas()\n    gc.collect()\n\n    df['mid_price*volume'] = df['mid_price_movement'] * df['volume']\n    df['harmonic_imbalance'] = df.eval('2 / ((1 / bid_size) + (1 / ask_size))')\n\n    for col in df.columns:\n        df[col] = df[col].replace([np.inf, -np.inf], 0)\n\n    return df\n\n# generate time & stock features\ndef other_features(df):\n    df[\"dom\"] = df[\"date_id\"] % 20\n    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60\n    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60\n\n    for key, value in global_stock_id_feats.items():\n        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n\n    return df\n","metadata":{"id":"W__K61JUqQyZ","execution":{"iopub.status.busy":"2023-12-20T06:11:15.851823Z","iopub.execute_input":"2023-12-20T06:11:15.852117Z","iopub.status.idle":"2023-12-20T06:11:15.877031Z","shell.execute_reply.started":"2023-12-20T06:11:15.852091Z","shell.execute_reply":"2023-12-20T06:11:15.876153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Stock weights for calculating imbalnce features\nweights = [\n    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n]\n\nweights = {int(k):v for k,v in enumerate(weights)}","metadata":{"id":"EB68X077qQyZ","execution":{"iopub.status.busy":"2023-12-20T06:11:15.878926Z","iopub.execute_input":"2023-12-20T06:11:15.879197Z","iopub.status.idle":"2023-12-20T06:11:15.891783Z","shell.execute_reply.started":"2023-12-20T06:11:15.879173Z","shell.execute_reply":"2023-12-20T06:11:15.890963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 Stock Classification Features","metadata":{"id":"zOhnuy3Ey5CX"}},{"cell_type":"markdown","source":"We first calculate ***correlation matrix*** of WAP, based on which we assign clusters to stocks. 1）After normalize the clusters, ranging from （-1, 1）, we try to construct ***stock classification features***. 2）We try to apply Stratifies K-Fold Cross Validation, so to speak carrying out CV by groups to train model ( LightGBM ) and fature selection ( REF ).","metadata":{"id":"YymmVYonzHT9"}},{"cell_type":"code","source":"# #Correlation Matrix of WAP\n# def calculate_daily_returns(stock_data):\n#     stock_data['return'] = stock_data['wap'].pct_change()\n#     return stock_data[['return', 'seconds_in_bucket']].dropna()  # Keep 'return' and 'seconds_in_bucket' columns\n\n# returns = df.groupby(['stock_id', 'date_id']).apply(calculate_daily_returns).reset_index()\n\n# # 2. Align the data for each stock by filling in the gaps (if any) and then concatenate the returns to form a matrix\n# # For this step, we will pivot the data so each stock has its own column, and each row represents a timestamp.\n# pivot_returns = returns.pivot_table(index=['date_id', 'seconds_in_bucket'],\n#                                     columns='stock_id',\n#                                     values='return')\n\n# # handle missing values by filling the average of all available\n# pivot_returns = pivot_returns.apply(lambda row: row.fillna(row.mean()), axis=1)\n\n# # 3. Compute the correlation matrix for all stocks\n# correlation_matrix = pivot_returns.corr()\n# #print(correlation_matrix)","metadata":{"id":"AQq5xtFWqQyZ","execution":{"iopub.status.busy":"2023-12-20T06:11:15.893088Z","iopub.execute_input":"2023-12-20T06:11:15.893431Z","iopub.status.idle":"2023-12-20T06:14:26.932352Z","shell.execute_reply.started":"2023-12-20T06:11:15.893399Z","shell.execute_reply":"2023-12-20T06:14:26.931545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Hierarchical clustering\n# Z = linkage(correlation_matrix, 'ward')\n# clusters = fcluster(Z, 10, criterion = 'maxclust')\n\n# #Assign clusters to stocks\n# stock_clusters = pd.DataFrame({'stock_id': correlation_matrix.index, 'cluster': clusters})\n# print(stock_clusters)","metadata":{"id":"w56y-wwyqQyZ","outputId":"b55dfefc-04a6-41c9-b40a-bca013b46a2e","execution":{"iopub.status.busy":"2023-12-20T06:14:26.933457Z","iopub.execute_input":"2023-12-20T06:14:26.933756Z","iopub.status.idle":"2023-12-20T06:14:26.948356Z","shell.execute_reply.started":"2023-12-20T06:14:26.933731Z","shell.execute_reply":"2023-12-20T06:14:26.947475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #Normalized stock cluster feature\n\n# def cluster_feature(df):\n#     #normalize\n#     scaler = MinMaxScaler(feature_range=(-1, 1))\n#     normalized_clusters = scaler.fit_transform(clusters.reshape(-1, 1))\n\n#     # Assign clusters to stocks\n\n#     # Assign clusters to stocks\n#     # 1. Extract cluster labels from hierarchical clustering\n#     df_clusters = pd.DataFrame({'stock_id': pivot_returns.columns, 'cluster_label': normalized_clusters.flatten()})\n\n#     # 2. Map cluster labels to each stock ID\n#     stock_id_to_cluster = dict(zip(df_clusters['stock_id'], df_clusters['cluster_label']))\n\n#     # 3. Add cluster labels to your original DataFrame\n#     df['cluster'] = df['stock_id'].map(stock_id_to_cluster)\n#     return df","metadata":{"id":"x8h88J3VqQya","execution":{"iopub.status.busy":"2023-12-20T06:14:26.950051Z","iopub.execute_input":"2023-12-20T06:14:26.950744Z","iopub.status.idle":"2023-12-20T06:14:26.957102Z","shell.execute_reply.started":"2023-12-20T06:14:26.950712Z","shell.execute_reply":"2023-12-20T06:14:26.956352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3 PCA_Weighed Average Features * 4","metadata":{"id":"tFhYZcZ10qn0"}},{"cell_type":"markdown","source":"In this section, we apply PCA on WAG and retrieve the first four elements as features for model training.","metadata":{"id":"M8AXF3Wf0wsO"}},{"cell_type":"code","source":"# from sklearn.decomposition import PCA\n\n# pca = PCA()\n# principal_components = pca.fit_transform(correlation_matrix)","metadata":{"id":"iU8zwMqGqQya","execution":{"iopub.status.busy":"2023-12-20T06:14:26.958068Z","iopub.execute_input":"2023-12-20T06:14:26.958406Z","iopub.status.idle":"2023-12-20T06:14:26.967705Z","shell.execute_reply.started":"2023-12-20T06:14:26.958375Z","shell.execute_reply":"2023-12-20T06:14:26.966906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #PCA-weighed average price features\n# def pca_wap_feature(df):\n\n#     #Focus on the first 4 components and save as DataFrame\n\n#     #Create a pivot table for wap\n#     price_pivot = df.pivot_table(index=['date_id', 'seconds_in_bucket'], columns='stock_id', values='wap')\n\n#     #Generate principal DataFrame\n#     principal_df = pd.DataFrame(data=principal_components,\n#                                 index=correlation_matrix.index,  # use stock_ids as the index\n#                                 columns=['PC'+str(i) for i in range(1, principal_components.shape[1] + 1)])\n\n#     #Ensure the ordering of stock_id in price_pivot and principal_pca is consistent\n#     ordered_columns = price_pivot.columns\n#     principal_df = principal_df.loc[ordered_columns].reset_index()\n\n#     #Handle NaN values and replace with 0\n#     price_pivot.fillna(0, inplace=True)\n#     principal_df.fillna(0, inplace=True)\n\n#     #Initialize a dataframe to hold the PCA_WAP values\n#     pca_wap_df = pd.DataFrame(index=price_pivot.index)\n\n#     #Compute 4 WAPs using PCA\n#     for i in range(1,5):\n#         pca_wap_df[f'PCA_WAP_{i}'] = (price_pivot.values * principal_df.set_index('stock_id')[f'PC{i}'].values).sum(axis=1)\n\n#     #Resetting index for merging purposes\n#     pca_wap_df = pca_wap_df.reset_index()\n\n#     #Merging the PCA_WAP columns with the initial dataset df\n#     df = df.merge(pca_wap_df, on=['date_id', 'seconds_in_bucket'], how='left')\n\n#     return df","metadata":{"id":"agZC68IhqQya","execution":{"iopub.status.busy":"2023-12-20T06:14:26.968679Z","iopub.execute_input":"2023-12-20T06:14:26.968948Z","iopub.status.idle":"2023-12-20T06:14:26.981699Z","shell.execute_reply.started":"2023-12-20T06:14:26.968925Z","shell.execute_reply":"2023-12-20T06:14:26.981001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4 Feature Generation Function","metadata":{"id":"BNXjHo4n1PyZ"}},{"cell_type":"code","source":"# generate all features\ndef generate_all_features(df):\n    cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\", \"target\"]]\n    df = df[cols]\n    df = imbalance_features(df)\n    df = other_features(df)\n    #df = cluster_feature(df)\n    #df = pca_wap_feature(df)\n\n    gc.collect()\n\n    feature_name = [i for i in df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\"]]\n\n    return df[feature_name]","metadata":{"id":"_sMo3LtAqQya","execution":{"iopub.status.busy":"2023-12-20T06:14:26.982750Z","iopub.execute_input":"2023-12-20T06:14:26.985527Z","iopub.status.idle":"2023-12-20T06:14:26.992695Z","shell.execute_reply.started":"2023-12-20T06:14:26.985503Z","shell.execute_reply":"2023-12-20T06:14:26.991949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3 Feature Generation","metadata":{"id":"UMtZod5f1US0"}},{"cell_type":"markdown","source":"### 3.1 Dataset Splitting","metadata":{"id":"6H_L5qiS3Fka"}},{"cell_type":"markdown","source":"When the code is running offline, the entire dataset will be used for training. When the code is running online, 90% data will be used for training and the rest will be used for validation.\nAdditionally, for memory saving, after data splitting, we delete the original dataset, so to speak, dataframe, ***df***.","metadata":{"id":"hU222Qwg1Yam"}},{"cell_type":"code","source":"# Check if the code is running in offline or online mode\nif is_offline:\n    # In offline mode, split the data into training and validation sets based on the split_day\n    df_train = df[df[\"date_id\"] <= split_day]\n    df_valid = df[df[\"date_id\"] > split_day]\n    print(\"Offline mode\")\n    print(f\"train : {df_train.shape}, valid : {df_valid.shape}\")\nelse:\n    # In online mode, use the entire dataset for training\n    df_train = df\n    print(\"Online mode\")\n\ndel df\ngc.collect()","metadata":{"id":"qq2Vi6Y8qQya","outputId":"4b681868-d773-41e1-89b6-3c7833dd7a8a","execution":{"iopub.status.busy":"2023-12-20T06:14:26.993617Z","iopub.execute_input":"2023-12-20T06:14:26.993842Z","iopub.status.idle":"2023-12-20T06:14:27.122970Z","shell.execute_reply.started":"2023-12-20T06:14:26.993822Z","shell.execute_reply":"2023-12-20T06:14:27.121948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2 Feature Generation","metadata":{"id":"S6vctrXf3Iuv"}},{"cell_type":"code","source":"%%time\nif is_train:\n    global_stock_id_feats = {\n        \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median()\n    }\n    if is_offline:\n        df_train_feats = generate_all_features(df_train)\n        print(\"Build Train Feats Finished.\")\n        df_valid_feats = generate_all_features(df_valid)\n        print(\"Build Valid Feats Finished.\")\n        df_valid_feats = reduce_mem_usage(df_valid_feats)\n    else:\n        df_train_feats = generate_all_features(df_train)\n        print(\"Build Online Train Feats Finished.\")\n\n    df_train_feats = reduce_mem_usage(df_train_feats)\n\nfeature_name = list(df_train_feats.columns)\nprint(f\"Feature length = {len(feature_name)}\")","metadata":{"id":"mxYW6TGBqQya","outputId":"0368edc2-231d-45c1-a8b7-9139c96829b4","execution":{"iopub.status.busy":"2023-12-20T06:18:56.091705Z","iopub.execute_input":"2023-12-20T06:18:56.092093Z","iopub.status.idle":"2023-12-20T06:20:24.724727Z","shell.execute_reply.started":"2023-12-20T06:18:56.092065Z","shell.execute_reply":"2023-12-20T06:20:24.723793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4 Feature Selection","metadata":{"id":"bQ12CZd03ey4"}},{"cell_type":"markdown","source":"### 4.1 CatBoost for Feature Selection: reducing features from 184 -> 100","metadata":{"id":"rUWIFSee3g1m"}},{"cell_type":"code","source":"del_list=[\n #'PCA_WAP_3',\n #'global_std_price',\n #'PCA_WAP_2'   \n #'dow',\n 'near_price_wap_imb',\n 'weighted_wap',\n 'near_price_diff_10',\n 'ask_price_wap_imb',\n 'dom',\n 'matched_size_ret_3',\n 'reference_price_ret_10',\n 'wap_diff_10',\n 'matched_size_ret_5',\n 'rolling_diff_bid_price_3',\n 'reference_price_far_price_imb',\n 'all_prices_skew',\n #'global_ptp_size',\n 'rolling_std_diff_ask_price_5',\n #'PCA_WAP_4',\n #'PCA_WAP_1',\n #'cluster',\n #'global_median_price',\n 'all_prices_mean',\n #'global_std_size',\n 'all_prices_kurt',\n 'stock_id',\n 'matched_size_shift_10',\n 'rolling_std_diff_bid_price_3',\n 'rolling_std_diff_bid_price_5',\n 'stock_weights',\n 'rolling_std_diff_ask_size_3',\n 'depth_pressure',\n 'spread_depth_ratio',\n 'bid_size_diff_10',\n 'near_price_bid_price_imb',\n #'global_ptp_price',\n 'far_price_near_price_imb',\n 'imbalance_buy_sell_flag_shift_3',\n 'rolling_std_diff_bid_price_10',\n 'ask_price_bid_price_reference_price_imb2',\n 'size_imbalance',\n 'mid_price',\n 'near_price',\n 'rolling_std_diff_ask_price_10',\n 'ask_price_bid_price_imb',\n 'rolling_diff_bid_size_10',\n 'bid_price_diff_2',\n 'matched_size_shift_5']","metadata":{"id":"qPHQFCPIwEBA","execution":{"iopub.status.busy":"2023-12-20T06:20:53.760697Z","iopub.execute_input":"2023-12-20T06:20:53.761574Z","iopub.status.idle":"2023-12-20T06:20:53.767613Z","shell.execute_reply.started":"2023-12-20T06:20:53.761541Z","shell.execute_reply":"2023-12-20T06:20:53.766693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if is_offline:\n    df_train_feats.drop(del_list,axis=1,inplace=True)\n    df_valid_feats.drop(del_list,axis=1,inplace=True)\nelse:\n    df_train_feats.drop(del_list,axis=1,inplace=True)\nfeature_name = list(df_train_feats.columns)\nprint(f\"Feature length = {len(feature_name)}\")","metadata":{"id":"vmc5Yi4TTFab","outputId":"8c3aa5ac-3c1c-4d87-f8f1-f5eefe83ba7e","execution":{"iopub.status.busy":"2023-12-20T06:20:57.756008Z","iopub.execute_input":"2023-12-20T06:20:57.756386Z","iopub.status.idle":"2023-12-20T06:21:00.002881Z","shell.execute_reply.started":"2023-12-20T06:20:57.756348Z","shell.execute_reply":"2023-12-20T06:21:00.001908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set LightGBM parameters\nlgb_params = {\n    \"objective\": \"mae\",\n    \"n_estimators\": 5000, #less estimators\n    \"num_leaves\": 256, #more leaves\n    \"subsample\": 0.6,\n    \"colsample_bytree\": 0.6,\n    \"learning_rate\": 0.00871, #larger learning rate(from 0.00871 to 0.00005)\n    'max_depth': 11,\n    \"n_jobs\": 4,\n    \"device\": \"gpu\",\n    \"verbosity\": -1,\n    \"importance_type\": \"gain\",\n}\n# Get feature names\nfeature_name = list(df_train_feats.columns)\nprint(f\"Feature length = {len(feature_name)}\")\n\n# Set up cross-validation parameters\nnum_folds = 5\nfold_size = 480 // num_folds #online mode\ngap = 5\n\n# Initialize lists to store models and scores\nmodels = []\nscores = []\n\n# Set model save path\nmodel_save_path = 'modelitos_para_despues'\nif not os.path.exists(model_save_path):\n    os.makedirs(model_save_path)\n\n# Get date IDs from the training data\ndate_ids = df_train['date_id'].values\n\n# Loop over folds for cross-validation\nfor i in range(num_folds):\n    start = i * fold_size\n    end = start + fold_size\n    \n    # Define the purged set ranges\n    purged_before_start = start - 2\n    purged_before_end = start + 2\n    purged_after_start = end - 2\n    purged_after_end = end + 2\n    \n    # Exclude the purged ranges from the test set\n    purged_set = ((date_ids >= purged_before_start) & (date_ids <= purged_before_end)) | \\\n                 ((date_ids >= purged_after_start) & (date_ids <= purged_after_end))\n    \n    # Define test_indices excluding the purged set\n    test_indices = (date_ids >= start) & (date_ids < end) & ~purged_set\n    train_indices = ~test_indices & ~purged_set\n\n    # Create fold-specific training and validation sets\n    df_fold_train = df_train_feats[train_indices]\n    df_fold_train_target = df_train['target'][train_indices]\n    df_fold_valid = df_train_feats[test_indices]\n    df_fold_valid_target = df_train['target'][test_indices]\n\n    print(f\"Fold {i+1} Model Training\")\n\n    # Train a LightGBM model for the current fold\n    lgb_model = lgb.LGBMRegressor(**lgb_params)\n    lgb_model.fit(\n        df_fold_train[feature_name],\n        df_fold_train_target,\n        eval_set=[(df_fold_valid[feature_name], df_fold_valid_target)],\n        callbacks=[\n            lgb.callback.early_stopping(stopping_rounds=100),\n            lgb.callback.log_evaluation(period=100),\n        ],\n    )\n\n    models.append(lgb_model)\n\n    # Save the model to a file\n    model_filename = os.path.join(model_save_path, f'doblez_{i+1}.txt')\n    lgb_model.booster_.save_model(model_filename)\n    print(f\"Model for fold {i+1} saved to {model_filename}\")\n\n    # Evaluate model performance on the validation set\n    fold_predictions = lgb_model.predict(df_fold_valid[feature_name])\n    fold_score = mean_absolute_error(fold_predictions, df_fold_valid_target)\n    scores.append(fold_score)\n    print(f\"Fold {i+1} MAE: {fold_score}\")\n\n    # Free up memory by deleting fold-specific variables\n    del df_fold_train, df_fold_train_target, df_fold_valid, df_fold_valid_target\n    gc.collect()\n\n# Calculate the average best iteration from all regular folds\naverage_best_iteration = int(np.mean([model.best_iteration_ for model in models]))\n\n# Update the lgb_params with the average best iteration\nfinal_model_params = lgb_params.copy()\nfinal_model_params['n_estimators'] = average_best_iteration\n\nprint(f\"Training final model with average best iteration: {average_best_iteration}\")\n\n# Train the final model on the entire dataset\nfinal_model = lgb.LGBMRegressor(**final_model_params)\nfinal_model.fit(\n    df_train_feats[feature_name],\n    df_train['target'],\n    callbacks=[\n        lgb.callback.log_evaluation(period=100),\n    ],\n)\n# Append the final model to the list of models\nmodels.append(final_model)\n\n# Append the final model to the list of models\nmodels.append(final_model)\n\n# Save the final model to a file\nfinal_model_filename = os.path.join(model_save_path, 'doblez-conjunto.txt')\nfinal_model.booster_.save_model(final_model_filename)\nprint(f\"Final model saved to {final_model_filename}\")\n\n# Now 'models' holds the trained models for each fold and 'scores' holds the validation scores\nprint(f\"Average MAE across all folds: {np.mean(scores)}\")\n\nif is_offline:\n    # offline predictions\n    df_valid_target = df_valid[\"target\"]\n    offline_predictions = final_model.predict(df_valid_feats[feature_name])\n    offline_score = mean_absolute_error(offline_predictions, df_valid_target)\n    print(f\"Offline Score {np.round(offline_score, 4)}\")","metadata":{"id":"jXVtNDMbEcYQ","outputId":"f85fedbe-4fe8-4dc4-d1db-884510333178","execution":{"iopub.status.busy":"2023-12-20T06:21:09.904801Z","iopub.execute_input":"2023-12-20T06:21:09.905650Z","iopub.status.idle":"2023-12-20T06:43:06.573008Z","shell.execute_reply.started":"2023-12-20T06:21:09.905618Z","shell.execute_reply":"2023-12-20T06:43:06.571478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def weighted_average(a):\n    w = []\n    n = len(a)\n    for j in range(1, n + 1):\n        j = 2 if j == 1 else j\n        w.append(1 / (2**(n + 1 - j)))\n    return w","metadata":{"execution":{"iopub.status.busy":"2023-12-20T06:43:06.573693Z","iopub.status.idle":"2023-12-20T06:43:06.574018Z","shell.execute_reply.started":"2023-12-20T06:43:06.573858Z","shell.execute_reply":"2023-12-20T06:43:06.573873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def zero_sum(prices, volumes):\n    std_error = np.sqrt(volumes)\n    step = np.sum(prices) / np.sum(std_error)\n    out = prices - std_error * step\n    return out\n\nif is_infer:\n    import optiver2023\n    env = optiver2023.make_env()\n    iter_test = env.iter_test()\n    counter = 0\n    y_min, y_max = -64, 64\n    qps, predictions = [], []\n    cache = pd.DataFrame()\n\n    # Weights for each fold model\n    model_weights = weighted_average(models)\n    \n    for (test, revealed_targets, sample_prediction) in iter_test:\n        now_time = time.time()\n        cache = pd.concat([cache, test], ignore_index=True, axis=0)\n        if counter > 0:\n            cache = cache.groupby(['stock_id']).tail(21).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n        feat = generate_all_features(cache)[-len(test):]\n        feat.drop(del_list,axis=1,inplace=True)\n\n        # Generate predictions for each model and calculate the weighted average\n        lgb_predictions = np.zeros(len(test))\n        for model, weight in zip(models, model_weights):\n            lgb_predictions += weight * model.predict(feat[feature_name])\n\n        lgb_predictions = zero_sum(lgb_predictions, test['bid_size'] + test['ask_size'])\n        clipped_predictions = np.clip(lgb_predictions, y_min, y_max)\n        sample_prediction['target'] = clipped_predictions\n        env.predict(sample_prediction)\n        counter += 1\n        qps.append(time.time() - now_time)\n        if counter % 10 == 0:\n            print(counter, 'qps:', np.mean(qps))\n\n    time_cost = 1.146 * np.mean(qps)\n    print(f\"The code will take approximately {np.round(time_cost, 4)} hours to reason about\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-20T06:43:06.575616Z","iopub.status.idle":"2023-12-20T06:43:06.575945Z","shell.execute_reply.started":"2023-12-20T06:43:06.575780Z","shell.execute_reply":"2023-12-20T06:43:06.575795Z"},"trusted":true},"execution_count":null,"outputs":[]}]}