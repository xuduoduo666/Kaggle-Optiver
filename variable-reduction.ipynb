{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-12-16T07:42:17.790018Z",
     "iopub.status.busy": "2023-12-16T07:42:17.789165Z",
     "iopub.status.idle": "2023-12-16T07:42:17.797251Z",
     "shell.execute_reply": "2023-12-16T07:42:17.796363Z",
     "shell.execute_reply.started": "2023-12-16T07:42:17.789983Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'polars'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KFold, TimeSeriesSplit\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpolars\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhierarchy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dendrogram, linkage, fcluster\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'polars'"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from itertools import combinations\n",
    "from warnings import simplefilter\n",
    "\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit\n",
    "import polars as pl\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "is_offline = True\n",
    "is_train = True\n",
    "is_infer = True\n",
    "max_lookback = np.nan\n",
    "split_day = 435"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T07:42:17.799289Z",
     "iopub.status.busy": "2023-12-16T07:42:17.798930Z",
     "iopub.status.idle": "2023-12-16T07:42:30.644304Z",
     "shell.execute_reply": "2023-12-16T07:42:30.643385Z",
     "shell.execute_reply.started": "2023-12-16T07:42:17.799255Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5237892, 17)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/optiver-trading-at-the-close/train.csv\")\n",
    "df = df.dropna(subset=[\"target\"]) \n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T07:42:30.688865Z",
     "iopub.status.busy": "2023-12-16T07:42:30.688591Z",
     "iopub.status.idle": "2023-12-16T07:42:30.702884Z",
     "shell.execute_reply": "2023-12-16T07:42:30.702054Z",
     "shell.execute_reply.started": "2023-12-16T07:42:30.688841Z"
    }
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=0):\n",
    "    \"\"\"\n",
    "    Iterate through all numeric columns of a dataframe and modify the data type\n",
    "    to reduce memory usage.\n",
    "    \"\"\"\n",
    "\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "\n",
    "    if verbose:\n",
    "        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
    "        decrease = 100 * (start_mem - end_mem) / start_mem\n",
    "        logger.info(f\"Decreased by {decrease:.2f}%\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T07:42:30.705616Z",
     "iopub.status.busy": "2023-12-16T07:42:30.705295Z",
     "iopub.status.idle": "2023-12-16T07:42:30.718456Z",
     "shell.execute_reply": "2023-12-16T07:42:30.717624Z",
     "shell.execute_reply.started": "2023-12-16T07:42:30.705591Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Numba for just-in-time (JIT) compilation and parallel processing\n",
    "from numba import njit, prange\n",
    "\n",
    "# Function to compute triplet imbalance in parallel using Numba\n",
    "@njit(parallel=True)\n",
    "def compute_triplet_imbalance(df_values, comb_indices):\n",
    "    num_rows = df_values.shape[0]\n",
    "    num_combinations = len(comb_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "\n",
    "    # Loop through all combinations of triplets\n",
    "    for i in prange(num_combinations):\n",
    "        a, b, c = comb_indices[i]\n",
    "        \n",
    "        # Loop through rows of the DataFrame\n",
    "        for j in range(num_rows):\n",
    "            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n",
    "            \n",
    "            # Prevent division by zero\n",
    "            if mid_val == min_val:\n",
    "                imbalance_features[j, i] = np.nan\n",
    "            else:\n",
    "                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n",
    "\n",
    "    return imbalance_features\n",
    "\n",
    "# Function to calculate triplet imbalance for given price data and a DataFrame\n",
    "def calculate_triplet_imbalance_numba(price, df):\n",
    "    # Convert DataFrame to numpy array for Numba compatibility\n",
    "    df_values = df[price].values\n",
    "    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n",
    "\n",
    "    # Calculate the triplet imbalance using the Numba-optimized function\n",
    "    features_array = compute_triplet_imbalance(df_values, comb_indices)\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "\n",
    "    return features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T07:42:30.719841Z",
     "iopub.status.busy": "2023-12-16T07:42:30.719576Z",
     "iopub.status.idle": "2023-12-16T07:42:30.744137Z",
     "shell.execute_reply": "2023-12-16T07:42:30.743171Z",
     "shell.execute_reply.started": "2023-12-16T07:42:30.719818Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate imbalance features\n",
    "def imbalance_features(df):\n",
    "    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "\n",
    "    # V1\n",
    "    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n",
    "    df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n",
    "    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n",
    "    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n",
    "    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n",
    "    # Create features for pairwise price imbalances\n",
    "    for c in combinations(prices, 2):\n",
    "        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n",
    "    # Calculate triplet imbalance features using the Numba-optimized function\n",
    "    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n",
    "        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n",
    "        df[triplet_feature.columns] = triplet_feature.values\n",
    "        \n",
    "        \n",
    "    # V2\n",
    "    df[\"stock_weights\"] = df[\"stock_id\"].map(weights)\n",
    "    df[\"weighted_wap\"] = df[\"stock_weights\"] * df[\"wap\"]\n",
    "    df['wap_momentum'] = df.groupby('stock_id')['weighted_wap'].pct_change(periods=6)\n",
    "    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n",
    "    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n",
    "    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n",
    "    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n",
    "    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n",
    "    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n",
    "    df['spread_depth_ratio'] = (df['ask_price'] - df['bid_price']) / (df['bid_size'] + df['ask_size'])\n",
    "    df['mid_price_movement'] = df['mid_price'].diff(periods=5).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
    "    df['micro_price'] = ((df['bid_price'] * df['ask_size']) + (df['ask_price'] * df['bid_size'])) / (df['bid_size'] + df['ask_size'])\n",
    "    df['relative_spread'] = (df['ask_price'] - df['bid_price']) / df['wap']\n",
    "    \n",
    "    #价量横截面统计特征（均值，标准差，偏度，峰度）\n",
    "    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n",
    "        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n",
    "        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n",
    "    \n",
    "    \n",
    "    # V3\n",
    "    # Calculate shifted and return features for specific columns\n",
    "    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n",
    "        for window in [1, 2, 3, 5, 10]:\n",
    "            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n",
    "            df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)\n",
    "                    \n",
    "            \n",
    "    # Calculate diff features for specific columns      \n",
    "    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size',\n",
    "                'wap', 'near_price', 'far_price']:#'weighted_wap','price_spread'\n",
    "        for window in [1, 2, 3, 5, 10]:\n",
    "            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n",
    "    \n",
    "    #V4\n",
    "    for window in [3, 5, 10]:\n",
    "        df[f'price_change_diff_{window}'] = df[f'bid_price_diff_{window}'] - df[f'ask_price_diff_{window}']\n",
    "        df[f'size_change_diff_{window}'] = df[f'bid_size_diff_{window}'] - df[f'ask_size_diff_{window}']\n",
    "    \n",
    "    #V5\n",
    "    pl_df = pl.from_pandas(df)\n",
    "\n",
    "    windows = [3, 5, 10]\n",
    "    columns = ['ask_price', 'bid_price', 'ask_size', 'bid_size']\n",
    "\n",
    "    group = [\"stock_id\"]\n",
    "    expressions = []\n",
    "    \n",
    "    for window in windows:\n",
    "        for col in columns:\n",
    "            rolling_mean_expr = (\n",
    "                pl.col(f\"{col}_diff_{window}\")\n",
    "                .rolling_mean(window)\n",
    "                .over(group)\n",
    "                .alias(f'rolling_diff_{col}_{window}')\n",
    "            )\n",
    "\n",
    "            rolling_std_expr = (\n",
    "                pl.col(f\"{col}_diff_{window}\")\n",
    "                .rolling_std(window)\n",
    "                .over(group)\n",
    "                .alias(f'rolling_std_diff_{col}_{window}')\n",
    "            )\n",
    "\n",
    "            expressions.append(rolling_mean_expr)\n",
    "            expressions.append(rolling_std_expr)\n",
    "\n",
    "    lazy_df = pl_df.lazy().with_columns(expressions)\n",
    "\n",
    "    pl_df = lazy_df.collect()\n",
    "\n",
    "    df = pl_df.to_pandas()\n",
    "    gc.collect()\n",
    "    \n",
    "    df['mid_price*volume'] = df['mid_price_movement'] * df['volume']\n",
    "    df['harmonic_imbalance'] = df.eval('2 / ((1 / bid_size) + (1 / ask_size))')\n",
    "    \n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].replace([np.inf, -np.inf], 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "# generate time & stock features\n",
    "def other_features(df):\n",
    "    df[\"dow\"] = df[\"date_id\"] % 5\n",
    "    df[\"dom\"] = df[\"date_id\"] % 20\n",
    "    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60\n",
    "    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60\n",
    "\n",
    "    for key, value in global_stock_id_feats.items():\n",
    "        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T07:42:30.745515Z",
     "iopub.status.busy": "2023-12-16T07:42:30.745210Z",
     "iopub.status.idle": "2023-12-16T07:42:30.757818Z",
     "shell.execute_reply": "2023-12-16T07:42:30.757074Z",
     "shell.execute_reply.started": "2023-12-16T07:42:30.745491Z"
    }
   },
   "outputs": [],
   "source": [
    "weights = [\n",
    "    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n",
    "    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n",
    "    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n",
    "    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n",
    "    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n",
    "    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n",
    "    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n",
    "    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n",
    "    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n",
    "    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n",
    "    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n",
    "    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n",
    "    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n",
    "    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n",
    "    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n",
    "    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n",
    "    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n",
    "]\n",
    "\n",
    "weights = {int(k):v for k,v in enumerate(weights)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T07:42:30.759119Z",
     "iopub.status.busy": "2023-12-16T07:42:30.758854Z",
     "iopub.status.idle": "2023-12-16T07:45:47.483015Z",
     "shell.execute_reply": "2023-12-16T07:45:47.481886Z",
     "shell.execute_reply.started": "2023-12-16T07:42:30.759096Z"
    }
   },
   "outputs": [],
   "source": [
    "#Correlation Matrix of WAP\n",
    "def calculate_daily_returns(stock_data):\n",
    "    stock_data['return'] = stock_data['wap'].pct_change()\n",
    "    return stock_data[['return', 'seconds_in_bucket']].dropna()  # Keep 'return' and 'seconds_in_bucket' columns\n",
    "\n",
    "returns = df.groupby(['stock_id', 'date_id']).apply(calculate_daily_returns).reset_index()\n",
    "\n",
    "# 2. Align the data for each stock by filling in the gaps (if any) and then concatenate the returns to form a matrix\n",
    "# For this step, we will pivot the data so each stock has its own column, and each row represents a timestamp.\n",
    "pivot_returns = returns.pivot_table(index=['date_id', 'seconds_in_bucket'], \n",
    "                                    columns='stock_id', \n",
    "                                    values='return')\n",
    "\n",
    "# handle missing values by filling the average of all available \n",
    "pivot_returns = pivot_returns.apply(lambda row: row.fillna(row.mean()), axis=1)\n",
    "\n",
    "# 3. Compute the correlation matrix for all stocks\n",
    "correlation_matrix = pivot_returns.corr()\n",
    "#print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T07:45:47.485151Z",
     "iopub.status.busy": "2023-12-16T07:45:47.484852Z",
     "iopub.status.idle": "2023-12-16T07:45:47.497742Z",
     "shell.execute_reply": "2023-12-16T07:45:47.496748Z",
     "shell.execute_reply.started": "2023-12-16T07:45:47.485119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     stock_id  cluster\n",
      "0           0        4\n",
      "1           1        7\n",
      "2           2        7\n",
      "3           3        4\n",
      "4           4        4\n",
      "..        ...      ...\n",
      "195       195        5\n",
      "196       196        9\n",
      "197       197        3\n",
      "198       198        5\n",
      "199       199        9\n",
      "\n",
      "[200 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Hierarchical clustering\n",
    "Z = linkage(correlation_matrix, 'ward')\n",
    "clusters = fcluster(Z, 10, criterion = 'maxclust')\n",
    "\n",
    "#Assign clusters to stocks\n",
    "stock_clusters = pd.DataFrame({'stock_id': correlation_matrix.index, 'cluster': clusters})\n",
    "print(stock_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T07:45:47.499892Z",
     "iopub.status.busy": "2023-12-16T07:45:47.499110Z",
     "iopub.status.idle": "2023-12-16T07:45:47.506655Z",
     "shell.execute_reply": "2023-12-16T07:45:47.505705Z",
     "shell.execute_reply.started": "2023-12-16T07:45:47.499854Z"
    }
   },
   "outputs": [],
   "source": [
    "#Normalized stock cluster feature\n",
    "\n",
    "def cluster_feature(df):\n",
    "    #normalize\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    normalized_clusters = scaler.fit_transform(clusters.reshape(-1, 1))\n",
    "\n",
    "    # Assign clusters to stocks\n",
    "    \n",
    "    # Assign clusters to stocks\n",
    "    # 1. Extract cluster labels from hierarchical clustering\n",
    "    df_clusters = pd.DataFrame({'stock_id': pivot_returns.columns, 'cluster_label': normalized_clusters.flatten()})\n",
    "\n",
    "    # 2. Map cluster labels to each stock ID\n",
    "    stock_id_to_cluster = dict(zip(df_clusters['stock_id'], df_clusters['cluster_label']))\n",
    "\n",
    "    # 3. Add cluster labels to your original DataFrame\n",
    "    df['cluster'] = df['stock_id'].map(stock_id_to_cluster)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T07:45:47.508303Z",
     "iopub.status.busy": "2023-12-16T07:45:47.507717Z",
     "iopub.status.idle": "2023-12-16T07:45:47.539679Z",
     "shell.execute_reply": "2023-12-16T07:45:47.538556Z",
     "shell.execute_reply.started": "2023-12-16T07:45:47.508276Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "principal_components = pca.fit_transform(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T07:45:47.541717Z",
     "iopub.status.busy": "2023-12-16T07:45:47.540915Z",
     "iopub.status.idle": "2023-12-16T07:45:47.553441Z",
     "shell.execute_reply": "2023-12-16T07:45:47.552359Z",
     "shell.execute_reply.started": "2023-12-16T07:45:47.541684Z"
    }
   },
   "outputs": [],
   "source": [
    "#PCA-weighed average price features\n",
    "def pca_wap_feature(df):\n",
    "    \n",
    "    #Focus on the first 4 components and save as DataFrame\n",
    "    \n",
    "    #Create a pivot table for wap\n",
    "    price_pivot = df.pivot_table(index=['date_id', 'seconds_in_bucket'], columns='stock_id', values='wap')\n",
    "    \n",
    "    #Generate principal DataFrame\n",
    "    principal_df = pd.DataFrame(data=principal_components, \n",
    "                                index=correlation_matrix.index,  # use stock_ids as the index\n",
    "                                columns=['PC'+str(i) for i in range(1, principal_components.shape[1] + 1)])\n",
    "    \n",
    "    #Ensure the ordering of stock_id in price_pivot and principal_pca is consistent\n",
    "    ordered_columns = price_pivot.columns\n",
    "    principal_df = principal_df.loc[ordered_columns].reset_index()\n",
    "    \n",
    "    #Handle NaN values and replace with 0\n",
    "    price_pivot.fillna(0, inplace=True)\n",
    "    principal_df.fillna(0, inplace=True)\n",
    "    \n",
    "    #Initialize a dataframe to hold the PCA_WAP values\n",
    "    pca_wap_df = pd.DataFrame(index=price_pivot.index)\n",
    "    \n",
    "    #Compute 4 WAPs using PCA\n",
    "    for i in range(1,5):\n",
    "        pca_wap_df[f'PCA_WAP_{i}'] = (price_pivot.values * principal_df.set_index('stock_id')[f'PC{i}'].values).sum(axis=1)\n",
    "    \n",
    "    #Resetting index for merging purposes\n",
    "    pca_wap_df = pca_wap_df.reset_index()\n",
    "    \n",
    "    #Merging the PCA_WAP columns with the initial dataset df\n",
    "    df = df.merge(pca_wap_df, on=['date_id', 'seconds_in_bucket'], how='left')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T07:45:47.555518Z",
     "iopub.status.busy": "2023-12-16T07:45:47.554648Z",
     "iopub.status.idle": "2023-12-16T07:45:47.563661Z",
     "shell.execute_reply": "2023-12-16T07:45:47.562342Z",
     "shell.execute_reply.started": "2023-12-16T07:45:47.555486Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate all features\n",
    "def generate_all_features(df):\n",
    "    cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\", \"target\"]]\n",
    "    df = df[cols]\n",
    "    df = imbalance_features(df)\n",
    "    df = other_features(df)\n",
    "    df = cluster_feature(df)\n",
    "    df = pca_wap_feature(df)\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    feature_name = [i for i in df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\"]]\n",
    "    \n",
    "    return df[feature_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T07:45:47.578073Z",
     "iopub.status.busy": "2023-12-16T07:45:47.573852Z",
     "iopub.status.idle": "2023-12-16T07:45:48.286819Z",
     "shell.execute_reply": "2023-12-16T07:45:48.285865Z",
     "shell.execute_reply.started": "2023-12-16T07:45:47.578025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offline mode\n",
      "train : (4742893, 17), valid : (494999, 17)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the code is running in offline or online mode\n",
    "if is_offline:\n",
    "    # In offline mode, split the data into training and validation sets based on the split_day\n",
    "    df_train = df[df[\"date_id\"] <= split_day]\n",
    "    df_valid = df[df[\"date_id\"] > split_day]\n",
    "    print(\"Offline mode\")\n",
    "    print(f\"train : {df_train.shape}, valid : {df_valid.shape}\")\n",
    "else:\n",
    "    # In online mode, use the entire dataset for training\n",
    "    df_train = df\n",
    "    print(\"Online mode\")\n",
    "    \n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T07:45:48.288420Z",
     "iopub.status.busy": "2023-12-16T07:45:48.288103Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if is_train:\n",
    "    global_stock_id_feats = {\n",
    "        \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n",
    "        \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
    "        \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
    "        \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n",
    "        \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
    "        \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
    "    }\n",
    "    if is_offline:\n",
    "        df_train_feats = generate_all_features(df_train)\n",
    "        print(\"Build Train Feats Finished.\")\n",
    "        df_valid_feats = generate_all_features(df_valid)\n",
    "        print(\"Build Valid Feats Finished.\")\n",
    "        df_valid_feats = reduce_mem_usage(df_valid_feats)\n",
    "    else:\n",
    "        df_train_feats = generate_all_features(df_train)\n",
    "        print(\"Build Online Train Feats Finished.\")\n",
    "\n",
    "    df_train_feats = reduce_mem_usage(df_train_feats)\n",
    "    \n",
    "feature_name = list(df_train_feats.columns)\n",
    "print(f\"Feature length = {len(feature_name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train procedure\n",
    "if is_train:\n",
    "    offline_split = df_train['date_id']>(split_day - 45)\n",
    "    df_offline_train = df_train_feats[~offline_split]\n",
    "    df_offline_valid = df_train_feats[offline_split]\n",
    "    df_offline_train_target = df_train['target'][~offline_split]\n",
    "    df_offline_valid_target = df_train['target'][offline_split]\n",
    "    df_train_target = df_train[\"target\"]\n",
    "    del df_train\n",
    "    gc.collect()\n",
    "    \n",
    "    ctb_params = dict(iterations=1200,\n",
    "                      learning_rate=1.0,\n",
    "                      depth=8,\n",
    "                      l2_leaf_reg=30,\n",
    "                      bootstrap_type='Bernoulli',\n",
    "                      subsample=0.66,\n",
    "                      loss_function='MAE',\n",
    "                      eval_metric = 'MAE',\n",
    "                      metric_period=100,\n",
    "                      od_type='Iter',\n",
    "                      od_wait=30,\n",
    "                      task_type='GPU',\n",
    "                      allow_writing_files=False,\n",
    "                      )\n",
    "    \n",
    "    print(\"Feature Elimination Performing.\")\n",
    "    ctb_model = CatBoostRegressor(**ctb_params)\n",
    "    summary = ctb_model.select_features(\n",
    "        df_offline_train[feature_name], df_offline_train_target,\n",
    "        eval_set=[(df_offline_valid[feature_name], df_offline_valid_target)],\n",
    "        features_for_select=feature_name,\n",
    "        num_features_to_select=len(feature_name)-24,    # Dropping from 124 to 100\n",
    "        steps=3,\n",
    "        algorithm=EFeaturesSelectionAlgorithm.RecursiveByShapValues,\n",
    "        shap_calc_type=EShapCalcType.Regular,\n",
    "        train_final_model=False,\n",
    "        plot=True,\n",
    "    )\n",
    "    \n",
    "    print(\"Valid Model Training on Selected Features Subset.\")\n",
    "    ctb_model = CatBoostRegressor(**ctb_params)\n",
    "    ctb_model.fit(\n",
    "        df_offline_train[summary['selected_features_names']], df_offline_train_target,\n",
    "        eval_set=[(df_offline_valid[summary['selected_features_names']], df_offline_valid_target)],\n",
    "        use_best_model=True,\n",
    "    )\n",
    "    \n",
    "    del df_offline_train, df_offline_valid, df_offline_train_target, df_offline_valid_target\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"Infer Model Training on Selected Features Subset.\")\n",
    "    infer_params = ctb_params.copy()\n",
    "    # CatBoost train best with Valid number of iterations\n",
    "    infer_params[\"iterations\"] = ctb_model.best_iteration_\n",
    "    infer_ctb_model = CatBoostRegressor(**infer_params)\n",
    "    infer_ctb_model.fit(df_train_feats[summary['selected_features_names']], df_train_target)\n",
    "    print(\"Infer Model Training on Selected Features Subset Complete.\")\n",
    "    \n",
    "    if is_offline:   \n",
    "        # Offline predictions\n",
    "        df_valid_target = df_valid[\"target\"]\n",
    "        offline_predictions = infer_ctb_model.predict(df_valid_feats[summary['selected_features_names']])\n",
    "        offline_score = mean_absolute_error(offline_predictions, df_valid_target)\n",
    "        print(f\"Offline Score {np.round(offline_score, 4)}\")\n",
    "        del df_valid, df_valid_feats\n",
    "        gc.collect()\n",
    "    \n",
    "    del df_train_feats\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary['eliminated_features_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = infer_ctb_model.get_feature_importance(prettified=True)\n",
    "\n",
    "plt.figure(figsize=(12, 20))\n",
    "sns.barplot(x=\"Importances\", y=\"Feature Id\", data=feat_importances)\n",
    "plt.title('CatBoost features importance:')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = infer_ctb_model.get_feature_importance(prettified=True)\n",
    "\n",
    "plt.figure(figsize=(12, 20))\n",
    "sns.barplot(x=\"Importances\", y=\"Feature Id\", data=feat_importances)\n",
    "plt.title('CatBoost features importance:')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import EFstrType\n",
    "feat_interactions = infer_ctb_model.get_feature_importance(type=EFstrType.Interaction, prettified=True)\n",
    "top_interactions = feat_interactions[:10]\n",
    "top_interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_interactions['First Feature Index'] = top_interactions['First Feature Index'].apply(lambda x: summary['selected_features_names'][x])\n",
    "top_interactions['Second Feature Index'] = top_interactions['Second Feature Index'].apply(lambda x: summary['selected_features_names'][x])\n",
    "top_interactions.columns = ['First Feature', 'Second Feature', 'Interaction']\n",
    "top_interactions"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4173551,
     "sourceId": 7212612,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30627,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
